---
published: true
---


## Wiser's Stack

# MVP
in the early days when we were still called WisePricer (our main product), we were mostly LAMP. We built our product around the Yii framework (PHP) with custom extensions and components to manage queues and background tasks. Everything ran on one web server and another batch server. We later introduced Gearman to our batch processes and logging components sending events to Loggly and our own ELK deployment.
Our batch queues were implemented with ActiveMQ and later when it couldn’t support our load, moved to a custom implementation of queues on Redis.

# Crawling and scraping
we first implemented a distributed web scraper using PHP. a daily batch job would export keywords from our products catalog to a queue, and PHP processes would start and pull keywords from the queue, scrape relevant sites and process the results, these results would later be uploaded to another queue for storing.
This custom setup allowed us to scale up easily by adding worker servers, either for scraping or processing. Each server manages its own workload by monitoring CPU and memory utilization and spawning new processes as needed.
Today we rely more and more on a new web scraping platform that allows non developers to build, deploy and monitor custom web scrapers for our client in with a rich UI interface. This platform was built in Java and is conceptually similar to Apache Storm.
To this day our Redis queues have been able to withstand a load of thousands of simultaneous connections and billions of daily queries but we are planning to migrate to a new distributed queueing system built with Node.js and RabbitMQ.

In tandem with targeted scraping we are also crawling entire websites. Our legacy crawling infrastructure was built in Python using Scrapy and custom components. Data is kept in MySQL. We are now in the process of migrating to a Java based in house product that will rely on our web scraping designer with data being kept in a non relational database.

Our scraping and crawling servers are hosted on Softlayer and AWS. on Amazon we are using spot instances, those are auto scaled and maintained by Spotinst.

We use DynamoDB tables for caching scraped results thanks to their ability to support thousands of connections per second.

To be able to scrape web pages that make heavy use on Javascript and to simplify building custom scrapers for such sites, we have been using PhantomJS with Sahi as a driver.
Common drivers for PhantomJS do not work well in our use case (hundreds of processes running on the same machine), thus we have implemented LXC for this purpose to be able to load, execute and release resources efficiently.

A new improved PhantomJS service is now in use, we built a Node.js service that provides an API to our PhantomJS cluster and is able to load balance requests based on number of open sessions per machine.


# Monitoring
We rely heavily on Statsd, we’re sticklers for metrics and building alerts on top of them. We use Librato to graph and set alerts for our metrics. We route some of our metrics to InfluxDB for better offline analysis and aggregations.
We use Newrelic to monitor the health of our servers and web transactions latency. We’ve been using it for profiling batch processes in production as well. 

# Data processing
Most of our data is kept in a legacy master-slave(s) mysql setup, while we now transition to a micro service architecture. Huge tables that require thousands of inserts a second are kept in DynamoDB (such as our product pricing history data). 
Hourly and daily ETLs are used to send data to Redshift. We use our DWH for reports, offline data analysis and running machine learning algorithms. In some cases data is sent to our DWH by a way of AWS Lambda functions triggered by DynamoDB table events. Data coming from these events is saved to S3 and then transferred to Redshift periodically.

# Custom Analytics
Data coming from our clients who have implemented our Javascript snippet in their site is routed through Kinesis, then saved to S3 and aggregated and saved on MySQL and Redshift.

# WisePricer 2.0
For our new line of products and services we are relying on Ruby, Node.js and Ember.js as the frontend framework. Micro services are generally written in Node. Events between services are collected and dispatches via a custom Service Hub on top of RabbitMQ.
New dashboards in our web applications have either Elasticsearch or Redshift as a data source, with the former being used when ‘freshness’ of data is important and the latter when is is not and heavy aggregations (either daily or hourly) are needed.

# Deployments and Testing
We automate our deployments with Ansible and make use of Jenkins to run our automatic tests. We combine manual QA work on legacy systems with automated testing through CircleCI on newer projects. Functional tests are ran using Behat for PHP and Cucumber for Ruby.
We use Github as our git repository, with commit and pull request notifications being sent to Slack as they happen.
